{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-07-14T20:08:01.740841Z"},"trusted":true},"outputs":[],"source":["!pip install -q -U bitsandbytes --no-index --find-links ../input/llm-detect-pip/\n","!pip install -q -U transformers --no-index --find-links ../input/llm-detect-pip/\n","!pip install -q -U tokenizers --no-index --find-links ../input/llm-detect-pip/\n","!pip install -q -U peft --no-index --find-links ../input/llm-detect-pip/"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!pip install transformers peft accelerate bitsandbytes \\\n","    -U --no-index --find-links /kaggle/input/lmsys-wheel-files"]},{"cell_type":"markdown","metadata":{},"source":["The work in this notebook is inspired by these notebooks:\n","* https://www.kaggle.com/code/ivanvybornov/llama3-8b-lgbm-tfidf\n","* https://www.kaggle.com/code/kishanvavdara/inference-llama-3-8b"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import time\n","from dataclasses import dataclass\n","from concurrent.futures import ThreadPoolExecutor\n","\n","import torch\n","import sklearn\n","import numpy as np\n","import pandas as pd\n","from transformers import Gemma2ForSequenceClassification, GemmaTokenizerFast, BitsAndBytesConfig\n","from transformers.data.data_collator import pad_without_fast_tokenizer_warning\n","from peft import PeftModel\n","import os"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from threading import Thread\n","import gc\n","import os\n","import io\n","import json\n","import random\n","import pickle\n","import zipfile\n","import datetime\n","import time\n","\n","import torch\n","import numpy as np\n","import pandas as pd\n","from transformers import AutoTokenizer, LlamaModel, LlamaForSequenceClassification, BitsAndBytesConfig\n","from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType\n","from torch.cuda.amp import autocast\n","from IPython.display import display\n","import torch.nn.functional as F\n","import tokenizers"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["assert torch.cuda.device_count() == 2\n"]},{"cell_type":"markdown","metadata":{},"source":["# infer"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["@dataclass\n","class Config:\n","    gemma_dir = '/kaggle/input/gemma-2/transformers/gemma-2-9b-it-4bit/1/gemma-2-9b-it-4bit'\n","    lora_dir = '/kaggle/input/73zap2gx/checkpoint-5600'\n","    max_length = 2048\n","    batch_size = 4\n","    device = torch.device(\"cuda\")    \n","    tta = False  # test time augmentation. <prompt>-<model-b's response>-<model-a's response>\n","    spread_max_length = False  # whether to apply max_length//3 on each input or max_length on the concatenated input\n","\n","cfg = Config()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def process_text(text: str) -> str:\n","    return \" \".join(eval(text, {\"null\": \"\"}))\n","\n","test.loc[:, 'prompt'] = test['prompt'].apply(process_text)\n","test.loc[:, 'response_a'] = test['response_a'].apply(process_text)\n","test.loc[:, 'response_b'] = test['response_b'].apply(process_text)\n","\n","display(test.head(5))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def tokenize(\n","    tokenizer, prompt, response_a, response_b, max_length=cfg.max_length, spread_max_length=cfg.spread_max_length\n","):\n","    prompt = [\"<prompt>: \" + p for p in prompt]\n","    response_a = [\"\\n\\n<response_a>: \" + r_a for r_a in response_a]\n","    response_b = [\"\\n\\n<response_b>: \" + r_b for r_b in response_b]\n","    if spread_max_length:\n","        prompt = tokenizer(prompt, max_length=max_length//3, truncation=True, padding=False).input_ids\n","        response_a = tokenizer(response_a, max_length=max_length//3, truncation=True, padding=False).input_ids\n","        response_b = tokenizer(response_b, max_length=max_length//3, truncation=True, padding=False).input_ids\n","        input_ids = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]\n","        attention_mask = [[1]* len(i) for i in input_ids]\n","    else:\n","        text = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]\n","        tokenized = tokenizer(text, max_length=max_length, truncation=True, padding=False)\n","        input_ids = tokenized.input_ids\n","        attention_mask = tokenized.attention_mask\n","    return input_ids, attention_mask"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["%%time\n","\n","tokenizer = GemmaTokenizerFast.from_pretrained(cfg.gemma_dir)\n","tokenizer.add_eos_token = True\n","tokenizer.padding_side = \"right\"\n","\n","data = pd.DataFrame()\n","data[\"id\"] = test[\"id\"]\n","data[\"input_ids\"], data[\"attention_mask\"] = tokenize(tokenizer, test[\"prompt\"], test[\"response_a\"], test[\"response_b\"])\n","data[\"length\"] = data[\"input_ids\"].apply(len)\n","\n","aug_data = pd.DataFrame()\n","aug_data[\"id\"] = test[\"id\"]\n","# swap response_a & response_b\n","aug_data['input_ids'], aug_data['attention_mask'] = tokenize(tokenizer, test[\"prompt\"], test[\"response_b\"], test[\"response_a\"])\n","aug_data[\"length\"] = aug_data[\"input_ids\"].apply(len)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Load base model on GPU 0\n","device_0 = torch.device('cuda:0')\n","model_0 = Gemma2ForSequenceClassification.from_pretrained(\n","    cfg.gemma_dir,\n","    device_map=device_0,\n","    use_cache=False,\n",")\n","\n","# Load base model on GPU 1\n","device_1 = torch.device('cuda:1')\n","model_1 = Gemma2ForSequenceClassification.from_pretrained(\n","    cfg.gemma_dir,\n","    device_map=device_1,\n","    use_cache=False,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model_0 = PeftModel.from_pretrained(model_0, cfg.lora_dir)\n","model_1 = PeftModel.from_pretrained(model_1, cfg.lora_dir)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["@torch.no_grad()\n","@torch.cuda.amp.autocast()\n","def inference(df, model, device, batch_size=cfg.batch_size, max_length=cfg.max_length):\n","    a_win, b_win, tie = [], [], []\n","    \n","    for start_idx in range(0, len(df), batch_size):\n","        end_idx = min(start_idx + batch_size, len(df))\n","        tmp = df.iloc[start_idx:end_idx]\n","        input_ids = tmp[\"input_ids\"].to_list()\n","        attention_mask = tmp[\"attention_mask\"].to_list()\n","        inputs = pad_without_fast_tokenizer_warning(\n","            tokenizer,\n","            {\"input_ids\": input_ids, \"attention_mask\": attention_mask},\n","            padding=\"longest\",\n","            pad_to_multiple_of=None,\n","            return_tensors=\"pt\",\n","        )\n","        outputs = model(**inputs.to(device))\n","        proba = outputs.logits.softmax(-1).cpu()\n","        \n","        a_win.extend(proba[:, 0].tolist())\n","        b_win.extend(proba[:, 1].tolist())\n","        tie.extend(proba[:, 2].tolist())\n","    \n","    df[\"winner_model_a\"] = a_win\n","    df[\"winner_model_b\"] = b_win\n","    df[\"winner_tie\"] = tie\n","    \n","    return df"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["st = time.time()\n","\n","# sort by input length to fully leverage dynaminc padding\n","data = data.sort_values(\"length\", ascending=False)\n","# the total #tokens in sub_1 and sub_2 should be more or less the same\n","sub_1 = data.iloc[0::2].copy()\n","sub_2 = data.iloc[1::2].copy()\n","\n","with ThreadPoolExecutor(max_workers=2) as executor:\n","    results = executor.map(inference, (sub_1, sub_2), (model_0, model_1), (device_0, device_1))\n","\n","result_df = pd.concat(list(results), axis=0)\n","proba = result_df[[\"winner_model_a\", \"winner_model_b\", \"winner_tie\"]].values\n","\n","print(f\"elapsed time: {time.time() - st}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["st = time.time()\n","\n","if cfg.tta:\n","    data = aug_data.sort_values(\"length\", ascending=False)  # sort by input length to boost speed\n","    sub_1 = data.iloc[0::2].copy()\n","    sub_2 = data.iloc[1::2].copy()\n","\n","    with ThreadPoolExecutor(max_workers=2) as executor:\n","        results = executor.map(inference, (sub_1, sub_2), (model_0, model_1), (device_0, device_1))\n","\n","    tta_result_df = pd.concat(list(results), axis=0)\n","    # recall TTA's order is flipped\n","    tta_proba = tta_result_df[[\"winner_model_b\", \"winner_model_a\", \"winner_tie\"]].values \n","    # average original result and TTA result.\n","    proba = (proba + tta_proba) / 2\n","\n","print(f\"elapsed time: {time.time() - st}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["result_df.loc[:, \"winner_model_a\"] = proba[:, 0]\n","result_df.loc[:, \"winner_model_b\"] = proba[:, 1]\n","result_df.loc[:, \"winner_tie\"] = proba[:, 2]\n","submission_df = result_df[[\"id\", 'winner_model_a', 'winner_model_b', 'winner_tie']]\n","submission_df.to_csv('submission.csv', index=False)\n","display(submission_df)"]},{"cell_type":"markdown","metadata":{},"source":["## Importing Libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')\n","sample_sub = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/sample_submission.csv')"]},{"cell_type":"markdown","metadata":{},"source":["## Tokenize"]},{"cell_type":"markdown","metadata":{},"source":["## Load model \n","> We load 1 model on each gpu.  "]},{"cell_type":"markdown","metadata":{},"source":["## Inference"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["TARGETS = ['winner_model_a', 'winner_model_b', 'winner_tie']\n","\n","sample_sub[TARGETS] = result_df[TARGETS]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["llama_preds = result_df[TARGETS].values"]},{"cell_type":"markdown","metadata":{},"source":["## LGBM + tfidf"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["TAG = 'lmsys-chatbot-arena'\n","RUNPOD = os.path.exists('/workspace/')\n","KAGGLE = not RUNPOD\n","if KAGGLE: \n","    print('kaggle')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["try:\n","    import pandas as pd\n","except:\n","    !pip install -q kaggle\n","    !pip install -q pandas matplotlib scipy joblib scikit-learn lightgbm \n","    !pip install -q protobuf \n","    !pip install -q numba"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["DATA = '/data/' if RUNPOD else 'data/' \\\n","        if not os.path.exists('/kaggle/') \\\n","            else '/kaggle/input/{}/'.format(TAG)\n","\n","if RUNPOD:\n","    if not os.path.exists('~/.kaggle/kaggle.json'):\n","        !mkdir -p ~/.kaggle\n","        !cp /workspace/kaggle.json ~/.kaggle/kaggle.json\n","        !chmod 600 /root/.kaggle/kaggle.json\n","\n","    if not os.path.exists('/workspace/' + TAG + '.zip'):\n","        !kaggle competitions download $TAG -p /workspace/ \n","        \n","    if not os.path.exists('/data/'):\n","        import zipfile\n","        zipfile.ZipFile('/workspace/' + TAG + '.zip').extractall('/data/')    "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["INPUT_PATH = '/kaggle/input/'  \n","MODEL_PATH = '/workspace/models/'; LOGITS_PATH = '/workspace/logits/'\n","MODEL_PATH = MODEL_PATH if not KAGGLE else '/kaggle/input/' \\\n","                + [e for e in os.listdir('/kaggle/input') if 'lsys-models' in e][0] + '/'\n","print(MODEL_PATH)\n","\n","CODE_PATH = MODEL_PATH if KAGGLE else '/workspace/'\n","SAVE_PATH = MODEL_PATH if not KAGGLE else ''"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["os.environ['TOKENIZERS_PARALLELISM'] = 'false'"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train = pd.read_csv(open(DATA + 'train.csv', 'r'))\n","test = pd.read_csv(open(DATA + 'test.csv', 'r'))\n","sample = pd.read_csv(DATA + 'sample_submission.csv')\n","print(len(train), len(test))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["params = {}\n","if False: \n","    pass;\n","    params['subsample'] = 30\n","else:\n","    params['fold'] = -1\n","\n","\n","params['n_epochs'] = 1\n","params['n_lgb'] = 1\n","params['model'] = 'microsoft/deberta-v3-small'"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# params = {}\n","FULL = params.get('fold', 0) < 0\n","N_FOLDS = int(params.get('n_folds', 3)); \n","FOLD = int(params.get('fold', 0))\n","SEED = int(params.get('seed', 3))\n","SS = int(params.get('subsample', 1))\n","\n","print(N_FOLDS, FOLD, SEED, SS)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from sklearn.model_selection import StratifiedKFold\n","\n","def get_folds(train): \n","    return list(StratifiedKFold(N_FOLDS, random_state = SEED, shuffle = True)\\\n","                    .split(X = np.zeros(len(train)), y = train.iloc[:, -3:].idxmax(1)))\n","\n","train_ids, test_ids = get_folds(train)[FOLD] if not FULL else [list(range(len(train))), []]\n","if SS > 1:\n","    train_ids, test_ids = train_ids[::SS], test_ids[::SS]\n","\n","print(len(train_ids), len(test_ids));  assert set(train_ids) & set(test_ids) == set() "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["torch.manual_seed(datetime.datetime.now().microsecond)\n","random.seed(datetime.datetime.now().microsecond)\n","np.random.seed(datetime.datetime.now().microsecond)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["TRAIN = False\n","INFER = True \n","SAVE = False"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import lightgbm as lgb\n","from sklearn.feature_extraction.text import CountVectorizer"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["LGB = True\n","TRAIN_LGB = TRAIN and LGB and params.get('n_lgb', 1) > 0\n","INFER_LGB = not TRAIN and LGB"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["cvec  = pickle.load(open(MODEL_PATH + 'cvec.pkl', 'rb'))\n","ccvec = pickle.load(open(MODEL_PATH + 'ccvec.pkl', 'rb'))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def symlog(x):\n","    return (np.sign(x) * np.log1p(np.abs(x))).astype(np.float32)\n","\n","def dense(x):\n","    x = np.asarray(x.astype(np.float32).todense())\n","    x = symlog(x)\n","    return x\n","\n","def get_features(df):\n","    pfeat = np.hstack([dense(v.transform(df[c])) \n","                for v in [cvec, ccvec]\n","                    for c in ['prompt', ]])\n","    afeat = np.hstack([dense(v.transform(df[c])) \n","                for c in ['response_a', ]\n","                    for v in [cvec, ccvec]\n","                ])\n","    bfeat = np.hstack([dense(v.transform(df[c])) \n","                for c in ['response_b', ]\n","                    for v in [cvec, ccvec]\n","                ])\n","    \n","    v = np.hstack([\n","          afeat - bfeat, np.abs(afeat - bfeat), \n","        ])\n","    try: \n","        v = v / (len(all_vote_models) if len(df) < len(train) else 1)\n","    except:\n","        pass\n","\n","    extras = []\n","    EXTRAS = ['\\n', '\\n\\n', '.', ' ', '\",\"']\n","    for e in EXTRAS:\n","        for c in ['prompt', 'response_a', 'response_b']:\n","            extras.append(df[c].str.count(e).values)\n","            \n","    extras.append(df[c].str.len())\n","    extras.append(df[c].str.split().apply(lambda x: len(x)))\n","    \n","    extras = np.stack(extras, axis = 1)\n","    extras = np.hstack([extras ** 0.5, np.log1p(extras)])\n","    return np.hstack([v, extras])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["lgb_models = pickle.load(open(MODEL_PATH + 'lgb_models.pkl', 'rb'))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["if INFER and params.get('n_lgb', 1) > 0:\n","    df = test\n","    yps = []; b = 1000\n","    for i in range(0, len(df), b):\n","        arr = get_features(df.iloc[i: i + b])\n","        ypms = []\n","        for model in lgb_models:\n","            ypms.append(model.predict_proba(arr))\n","        yps.append(np.stack(ypms).mean(0))\n","        print('.', end = '')\n","        \n","        if len(yps) % 2 == 0:\n","            gc.collect()\n","    print()\n","\n","    yp = np.concatenate(yps)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["lgb_preds = yp"]},{"cell_type":"markdown","metadata":{},"source":["## Blend predictions\n","\n","$\\operatorname{preds} = 0.12 \\cdot \\operatorname{lgbm boosting preds} + 0.8 \\cdot \\operatorname{llama preds}$\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["lgb_wt = 0.6\n","preds = lgb_wt * lgb_preds + (1 - lgb_wt) * llama_preds"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["out = pd.DataFrame(preds, index=df.id, columns=train.columns[-3:])\n","display(out.head())"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["out.to_csv('submission.csv')"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"databundleVersionId":8346466,"sourceId":66631,"sourceType":"competition"},{"datasetId":4946449,"sourceId":8330401,"sourceType":"datasetVersion"},{"datasetId":5034873,"sourceId":8449074,"sourceType":"datasetVersion"},{"datasetId":5297895,"sourceId":8897601,"sourceType":"datasetVersion"},{"datasetId":5369301,"sourceId":8926343,"sourceType":"datasetVersion"},{"sourceId":148861315,"sourceType":"kernelVersion"},{"sourceId":187768984,"sourceType":"kernelVersion"},{"modelId":86587,"modelInstanceId":63082,"sourceId":75103,"sourceType":"modelInstanceVersion"}],"dockerImageVersionId":30699,"isGpuEnabled":true,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
