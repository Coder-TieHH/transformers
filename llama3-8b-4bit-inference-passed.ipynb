{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fcf176e",
   "metadata": {
    "papermill": {
     "duration": 0.011903,
     "end_time": "2024-07-29T07:45:55.949072",
     "exception": false,
     "start_time": "2024-07-29T07:45:55.937169",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Note\n",
    "- [Training script](https://www.kaggle.com/code/shelterw/training-llama3-8b-4-bit-qlora-sft)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf0285b",
   "metadata": {
    "papermill": {
     "duration": 0.01062,
     "end_time": "2024-07-29T07:45:55.970839",
     "exception": false,
     "start_time": "2024-07-29T07:45:55.960219",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55217287",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-29T07:45:55.994064Z",
     "iopub.status.busy": "2024-07-29T07:45:55.993737Z",
     "iopub.status.idle": "2024-07-29T07:45:56.005493Z",
     "shell.execute_reply": "2024-07-29T07:45:56.004582Z"
    },
    "papermill": {
     "duration": 0.0258,
     "end_time": "2024-07-29T07:45:56.007513",
     "exception": false,
     "start_time": "2024-07-29T07:45:55.981713",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input\n"
     ]
    }
   ],
   "source": [
    "cd /kaggle/input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03c603dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-29T07:45:56.030936Z",
     "iopub.status.busy": "2024-07-29T07:45:56.030683Z",
     "iopub.status.idle": "2024-07-29T07:46:50.817308Z",
     "shell.execute_reply": "2024-07-29T07:46:50.816251Z"
    },
    "papermill": {
     "duration": 54.810859,
     "end_time": "2024-07-29T07:46:50.829570",
     "exception": false,
     "start_time": "2024-07-29T07:45:56.018711",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished!\n"
     ]
    }
   ],
   "source": [
    "!pip install -q -U bitsandbytes --no-index --find-links ./package-file\n",
    "!pip install -q -U transformers --no-index --find-links ./llm-detect-pip\n",
    "!pip install -q -U tokenizers --no-index --find-links ./llm-detect-pip\n",
    "!pip install -q -U peft --no-index --find-links ./llm-detect-pip\n",
    "print('finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30516d04",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-29T07:46:50.853809Z",
     "iopub.status.busy": "2024-07-29T07:46:50.853143Z",
     "iopub.status.idle": "2024-07-29T07:46:51.936189Z",
     "shell.execute_reply": "2024-07-29T07:46:51.935062Z"
    },
    "papermill": {
     "duration": 1.097968,
     "end_time": "2024-07-29T07:46:51.938648",
     "exception": false,
     "start_time": "2024-07-29T07:46:50.840680",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jul 29 07:46:51 2024       \r\n",
      "+-----------------------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.4     |\r\n",
      "|-----------------------------------------+------------------------+----------------------+\r\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                                         |                        |               MIG M. |\r\n",
      "|=========================================+========================+======================|\r\n",
      "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\r\n",
      "| N/A   34C    P8              9W /   70W |       1MiB /  15360MiB |      0%      Default |\r\n",
      "|                                         |                        |                  N/A |\r\n",
      "+-----------------------------------------+------------------------+----------------------+\r\n",
      "|   1  Tesla T4                       Off |   00000000:00:05.0 Off |                    0 |\r\n",
      "| N/A   33C    P8              9W /   70W |       1MiB /  15360MiB |      0%      Default |\r\n",
      "|                                         |                        |                  N/A |\r\n",
      "+-----------------------------------------+------------------------+----------------------+\r\n",
      "                                                                                         \r\n",
      "+-----------------------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                              |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\r\n",
      "|        ID   ID                                                               Usage      |\r\n",
      "|=========================================================================================|\r\n",
      "|  No running processes found                                                             |\r\n",
      "+-----------------------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05ce6b20",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-29T07:46:51.964453Z",
     "iopub.status.busy": "2024-07-29T07:46:51.964131Z",
     "iopub.status.idle": "2024-07-29T07:47:12.535834Z",
     "shell.execute_reply": "2024-07-29T07:47:12.534878Z"
    },
    "papermill": {
     "duration": 20.587765,
     "end_time": "2024-07-29T07:47:12.538278",
     "exception": false,
     "start_time": "2024-07-29T07:46:51.950513",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-29 07:47:03.006258: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-29 07:47:03.006371: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-29 07:47:03.149860: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from torch.cuda.amp import autocast\n",
    "from dataclasses import dataclass\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from threading import Thread\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig, DataCollatorForSeq2Seq\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorWithPadding, AutoModelForSequenceClassification\n",
    "from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType \n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
    "from transformers import BitsAndBytesConfig, LlamaForCausalLM, LlamaModel, LlamaPreTrainedModel\n",
    "from transformers.data.data_collator import pad_without_fast_tokenizer_warning\n",
    "from transformers import set_seed\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(True)\n",
    "torch.backends.cuda.enable_flash_sdp(True)\n",
    "assert torch.cuda.device_count() == 2, \"Sorry - multi-GPU required!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5aa77baf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-29T07:47:12.563327Z",
     "iopub.status.busy": "2024-07-29T07:47:12.562320Z",
     "iopub.status.idle": "2024-07-29T07:47:12.567181Z",
     "shell.execute_reply": "2024-07-29T07:47:12.566292Z"
    },
    "papermill": {
     "duration": 0.018986,
     "end_time": "2024-07-29T07:47:12.569070",
     "exception": false,
     "start_time": "2024-07-29T07:47:12.550084",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = './llama-3-8b-instruct-bnb-4bit/pytorch/default/1/llama-3-8b-Instruct-bnb-4bit'\n",
    "WEIGHTS_PATH = './train10000-bs8-ep1/train10000_bs8_ep1/'\n",
    "MAX_LENGTH = 2400\n",
    "BATCH_SIZE = 2\n",
    "DEVICE = torch.device(\"cuda\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0120e4",
   "metadata": {
    "papermill": {
     "duration": 0.011048,
     "end_time": "2024-07-29T07:47:12.591407",
     "exception": false,
     "start_time": "2024-07-29T07:47:12.580359",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Prepare Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1ff52bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-29T07:47:12.615517Z",
     "iopub.status.busy": "2024-07-29T07:47:12.614980Z",
     "iopub.status.idle": "2024-07-29T07:47:12.634699Z",
     "shell.execute_reply": "2024-07-29T07:47:12.633923Z"
    },
    "papermill": {
     "duration": 0.034163,
     "end_time": "2024-07-29T07:47:12.636822",
     "exception": false,
     "start_time": "2024-07-29T07:47:12.602659",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('./lmsys-chatbot-arena/test.csv')\n",
    "def tokenize(example, tokenizer):\n",
    "    prompt = tokenizer('<prompt>: ' + \" \".join(eval(example['prompt'], {\"null\": \"\"})), add_special_tokens=False)[\"input_ids\"]\n",
    "    response_a = tokenizer('\\n\\n<response_a>: ' + \" \".join(eval(example['response_a'], {\"null\": \"\"})), add_special_tokens=False)[\"input_ids\"]\n",
    "    response_b = tokenizer('\\n\\n<response_b>: ' + \" \".join(eval(example['response_b'], {\"null\": \"\"})), add_special_tokens=False)[\"input_ids\"]\n",
    "    if len(prompt+response_a+response_b) > MAX_LENGTH:\n",
    "        prompt = tokenizer('<prompt>: ' + eval(example['prompt'], {\"null\": \"\"})[-1], add_special_tokens=False)[\"input_ids\"][:400]\n",
    "        response_a = tokenizer('\\n\\n<response_a>: ' + eval(example['response_a'], {\"null\": \"\"})[-1], add_special_tokens=False)[\"input_ids\"][:1000]\n",
    "        response_b = tokenizer('\\n\\n<response_b>: ' + eval(example['response_b'], {\"null\": \"\"})[-1], add_special_tokens=False)[\"input_ids\"][:1000]\n",
    "    extra_prompt = tokenizer('\\n\\n---------\\nWhich is the better response for the prompt ? a or b or tie ?\\n\\nAnswer: ', add_special_tokens=False)[\"input_ids\"]\n",
    "    label_token_id = [128250]\n",
    "    input_ids = [tokenizer.bos_token_id] + prompt + response_a + response_b + extra_prompt + label_token_id + [tokenizer.eos_token_id]\n",
    "    attention_mask = len(input_ids)*[1]\n",
    "    labels = [-100]* len([tokenizer.bos_token_id] + prompt + response_a + response_b + extra_prompt) + label_token_id + [tokenizer.eos_token_id]\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa3dce5",
   "metadata": {
    "papermill": {
     "duration": 0.011289,
     "end_time": "2024-07-29T07:47:12.660188",
     "exception": false,
     "start_time": "2024-07-29T07:47:12.648899",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d502aae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-29T07:47:12.684710Z",
     "iopub.status.busy": "2024-07-29T07:47:12.683900Z",
     "iopub.status.idle": "2024-07-29T07:47:13.539710Z",
     "shell.execute_reply": "2024-07-29T07:47:13.538854Z"
    },
    "papermill": {
     "duration": 0.871173,
     "end_time": "2024-07-29T07:47:13.542825",
     "exception": false,
     "start_time": "2024-07-29T07:47:12.671652",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7ed5a3ce20540cebbe87a8f39e0aa96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 658 ms, sys: 59.6 ms, total: 718 ms\n",
      "Wall time: 846 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'prompt', 'response_a', 'response_b', 'input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 3\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "tokenizer = AutoTokenizer.from_pretrained(WEIGHTS_PATH)\n",
    "LABEL_IDS = [tokenizer(i, add_special_tokens=False)[\"input_ids\"][0] for i in ['a', 'b', 'tie']]\n",
    "def load_data(df, tokenizer):\n",
    "    raw_datasets = Dataset.from_pandas(df)\n",
    "    tokenized_datasets = raw_datasets.map(\n",
    "        tokenize, \n",
    "        # remove_columns=raw_datasets.column_names,\n",
    "        fn_kwargs={'tokenizer': tokenizer},\n",
    "    )\n",
    "    return tokenized_datasets\n",
    "test_ds = load_data(test, tokenizer)\n",
    "test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e5d39ed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-29T07:47:13.568913Z",
     "iopub.status.busy": "2024-07-29T07:47:13.568083Z",
     "iopub.status.idle": "2024-07-29T07:47:13.608851Z",
     "shell.execute_reply": "2024-07-29T07:47:13.607943Z"
    },
    "papermill": {
     "duration": 0.055702,
     "end_time": "2024-07-29T07:47:13.610768",
     "exception": false,
     "start_time": "2024-07-29T07:47:13.555066",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prompt</th>\n",
       "      <th>response_a</th>\n",
       "      <th>response_b</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "      <th>labels</th>\n",
       "      <th>max_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>136060</td>\n",
       "      <td>[\"I have three oranges today, I ate an orange ...</td>\n",
       "      <td>[\"You have two oranges today.\"]</td>\n",
       "      <td>[\"You still have three oranges. Eating an oran...</td>\n",
       "      <td>[128000, 8085, 15091, 27916, 358, 617, 2380, 8...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[-100, -100, -100, -100, -100, -100, -100, -10...</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>211333</td>\n",
       "      <td>[\"You are a mediator in a heated political deb...</td>\n",
       "      <td>[\"Thank you for sharing the details of the sit...</td>\n",
       "      <td>[\"Mr Reddy and Ms Blue both have valid points ...</td>\n",
       "      <td>[128000, 8085, 15091, 27916, 1472, 527, 264, 6...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[-100, -100, -100, -100, -100, -100, -100, -10...</td>\n",
       "      <td>469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1233961</td>\n",
       "      <td>[\"How to initialize the classification head wh...</td>\n",
       "      <td>[\"When you want to initialize the classificati...</td>\n",
       "      <td>[\"To initialize the classification head when p...</td>\n",
       "      <td>[128000, 8085, 15091, 27916, 2650, 311, 9656, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[-100, -100, -100, -100, -100, -100, -100, -10...</td>\n",
       "      <td>1593</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                             prompt  \\\n",
       "0   136060  [\"I have three oranges today, I ate an orange ...   \n",
       "1   211333  [\"You are a mediator in a heated political deb...   \n",
       "2  1233961  [\"How to initialize the classification head wh...   \n",
       "\n",
       "                                          response_a  \\\n",
       "0                    [\"You have two oranges today.\"]   \n",
       "1  [\"Thank you for sharing the details of the sit...   \n",
       "2  [\"When you want to initialize the classificati...   \n",
       "\n",
       "                                          response_b  \\\n",
       "0  [\"You still have three oranges. Eating an oran...   \n",
       "1  [\"Mr Reddy and Ms Blue both have valid points ...   \n",
       "2  [\"To initialize the classification head when p...   \n",
       "\n",
       "                                           input_ids  \\\n",
       "0  [128000, 8085, 15091, 27916, 358, 617, 2380, 8...   \n",
       "1  [128000, 8085, 15091, 27916, 1472, 527, 264, 6...   \n",
       "2  [128000, 8085, 15091, 27916, 2650, 311, 9656, ...   \n",
       "\n",
       "                                      attention_mask  \\\n",
       "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "\n",
       "                                              labels  max_len  \n",
       "0  [-100, -100, -100, -100, -100, -100, -100, -10...       82  \n",
       "1  [-100, -100, -100, -100, -100, -100, -100, -10...      469  \n",
       "2  [-100, -100, -100, -100, -100, -100, -100, -10...     1593  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = test_ds.to_pandas()\n",
    "data[\"max_len\"] = data[\"input_ids\"].apply(len)\n",
    "data[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb162c36",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-29T07:47:13.636353Z",
     "iopub.status.busy": "2024-07-29T07:47:13.636083Z",
     "iopub.status.idle": "2024-07-29T07:47:13.642459Z",
     "shell.execute_reply": "2024-07-29T07:47:13.641597Z"
    },
    "papermill": {
     "duration": 0.021556,
     "end_time": "2024-07-29T07:47:13.644486",
     "exception": false,
     "start_time": "2024-07-29T07:47:13.622930",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([128000,   8085,  15091,  27916,    358,    617,   2380,  85138,\n",
       "         3432,     11,    358,  30912,    459,  19087,  13985,     13,\n",
       "         2650,   1690,  85138,    656,    358,    617,     30,    271,\n",
       "           27,   2376,   4404,  27916,   1472,    617,   1403,  85138,\n",
       "         3432,     13,    271,     27,   2376,    890,  27916,   1472,\n",
       "         2103,    617,   2380,  85138,     13,  60638,    459,  19087,\n",
       "        13985,   1587,    539,   7958,    279,   1396,    315,  85138,\n",
       "          499,    617,   3432,     13,    271,  29547,  23956,    374,\n",
       "          279,   2731,   2077,    369,    279,  10137,    949,    264,\n",
       "          477,    293,    477,  18623,  24688,  16533,     25,    220,\n",
       "       128250, 128009], dtype=int32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['input_ids'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "10a86526",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-29T07:47:13.670335Z",
     "iopub.status.busy": "2024-07-29T07:47:13.670035Z",
     "iopub.status.idle": "2024-07-29T07:47:13.674654Z",
     "shell.execute_reply": "2024-07-29T07:47:13.673864Z"
    },
    "papermill": {
     "duration": 0.020105,
     "end_time": "2024-07-29T07:47:13.676782",
     "exception": false,
     "start_time": "2024-07-29T07:47:13.656677",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><prompt>: I have three oranges today, I ate an orange yesterday. How many oranges do I have?\n",
      "\n",
      "<response_a>: You have two oranges today.\n",
      "\n",
      "<response_b>: You still have three oranges. Eating an orange yesterday does not affect the number of oranges you have today.\n",
      "\n",
      "---------\n",
      "Which is the better response for the prompt? a or b or tie?\n",
      "\n",
      "Answer: <|reserved_special_token_245|><|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(data[\"input_ids\"][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f1689e",
   "metadata": {
    "papermill": {
     "duration": 0.012259,
     "end_time": "2024-07-29T07:47:13.701254",
     "exception": false,
     "start_time": "2024-07-29T07:47:13.688995",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Load model \n",
    "We load 1 model on each gpu.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "79a80a68",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-29T07:47:13.727256Z",
     "iopub.status.busy": "2024-07-29T07:47:13.726986Z",
     "iopub.status.idle": "2024-07-29T07:47:13.739764Z",
     "shell.execute_reply": "2024-07-29T07:47:13.739006Z"
    },
    "papermill": {
     "duration": 0.028176,
     "end_time": "2024-07-29T07:47:13.741619",
     "exception": false,
     "start_time": "2024-07-29T07:47:13.713443",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Llama3ForSFT(LlamaPreTrainedModel):\n",
    "    _tied_weights_keys = [\"lm_head.weight\"]\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.model = LlamaModel(config)\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "        self.post_init()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids= None,\n",
    "        attention_mask= None,\n",
    "        position_ids = None,\n",
    "        past_key_values= None,\n",
    "        inputs_embeds= None,\n",
    "        labels= None,\n",
    "        use_cache= None,\n",
    "        output_attentions= None,\n",
    "        output_hidden_states = None,\n",
    "        return_dict= None,\n",
    "        cache_position = None,\n",
    "    ):\n",
    "        outputs = self.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "            cache_position=cache_position,\n",
    "        )\n",
    "        hidden_states = outputs[0]\n",
    "        if self.config.pretraining_tp > 1:\n",
    "            lm_head_slices = self.lm_head.weight.split(self.vocab_size // self.config.pretraining_tp, dim=0)\n",
    "            logits = [F.linear(hidden_states, lm_head_slices[i]) for i in range(self.config.pretraining_tp)]\n",
    "            logits = torch.cat(logits, dim=-1)\n",
    "        else:\n",
    "            logits = self.lm_head(hidden_states)\n",
    "        logits = logits.float()\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # Shift so that tokens < n predict n\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            # Flatten the tokens\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            shift_logits = shift_logits.view(-1, self.config.vocab_size)\n",
    "            shift_labels = shift_labels.view(-1)\n",
    "            # Enable model parallelism\n",
    "            shift_labels = shift_labels.to(shift_logits.device)\n",
    "\n",
    "            fake_label_tokens_ids = torch.tensor([128250],device=shift_labels.device)\n",
    "            label_tokens_ids = torch.tensor(LABEL_IDS,device=shift_labels.device)\n",
    "#             index_mapping = {value.item(): idx for idx, value in enumerate(label_tokens_ids)}\n",
    "#             true_labels = shift_labels[torch.isin(shift_labels, label_tokens_ids)]\n",
    "#             true_labels = torch.tensor([index_mapping[label.item()] for label in true_labels], device=true_labels.device)\n",
    "            true_logits = shift_logits[torch.isin(shift_labels, fake_label_tokens_ids)][:,label_tokens_ids]\n",
    "#             loss = loss_fct(true_logits, true_labels)\n",
    "\n",
    "        return CausalLMOutputWithPast(\n",
    "            loss=loss,\n",
    "            logits=true_logits,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "60521326",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-29T07:47:13.767701Z",
     "iopub.status.busy": "2024-07-29T07:47:13.767418Z",
     "iopub.status.idle": "2024-07-29T07:48:11.400789Z",
     "shell.execute_reply": "2024-07-29T07:48:11.399790Z"
    },
    "papermill": {
     "duration": 57.649353,
     "end_time": "2024-07-29T07:48:11.403282",
     "exception": false,
     "start_time": "2024-07-29T07:47:13.753929",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load base model on GPU 0\n",
    "device0 = torch.device('cuda:0')\n",
    "base_model_0 = Llama3ForSFT.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    use_cache=False,\n",
    "    device_map='cuda:0',\n",
    ")\n",
    "# Load base model on GPU 1\n",
    "device1 = torch.device('cuda:1')\n",
    "base_model_1 = Llama3ForSFT.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    use_cache=False,\n",
    "    device_map='cuda:1',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67278b5a",
   "metadata": {
    "papermill": {
     "duration": 0.012111,
     "end_time": "2024-07-29T07:48:11.428201",
     "exception": false,
     "start_time": "2024-07-29T07:48:11.416090",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Load weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22ebbb14",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-29T07:48:11.453937Z",
     "iopub.status.busy": "2024-07-29T07:48:11.453613Z",
     "iopub.status.idle": "2024-07-29T07:48:26.268619Z",
     "shell.execute_reply": "2024-07-29T07:48:26.267710Z"
    },
    "papermill": {
     "duration": 14.830582,
     "end_time": "2024-07-29T07:48:26.270896",
     "exception": false,
     "start_time": "2024-07-29T07:48:11.440314",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Llama3ForSFT(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(128256, 4096)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaSdpaAttention(\n",
       "              (q_proj): Linear4bit(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): Linear4bit(\n",
       "                in_features=4096, out_features=1024, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (v_proj): Linear4bit(\n",
       "                in_features=4096, out_features=1024, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "              (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get peft\n",
    "model_0 = PeftModel.from_pretrained(base_model_0, model_id=WEIGHTS_PATH).to(device0) \n",
    "model_0.eval()\n",
    "\n",
    "model_1 = PeftModel.from_pretrained(base_model_1, model_id=WEIGHTS_PATH).to(device1)\n",
    "model_1.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c618a3f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-29T07:48:26.298360Z",
     "iopub.status.busy": "2024-07-29T07:48:26.298065Z",
     "iopub.status.idle": "2024-07-29T07:48:27.498687Z",
     "shell.execute_reply": "2024-07-29T07:48:27.497451Z"
    },
    "papermill": {
     "duration": 1.217392,
     "end_time": "2024-07-29T07:48:27.501251",
     "exception": false,
     "start_time": "2024-07-29T07:48:26.283859",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jul 29 07:48:27 2024       \r\n",
      "+-----------------------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.4     |\r\n",
      "|-----------------------------------------+------------------------+----------------------+\r\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                                         |                        |               MIG M. |\r\n",
      "|=========================================+========================+======================|\r\n",
      "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\r\n",
      "| N/A   42C    P0             26W /   70W |    5755MiB /  15360MiB |      0%      Default |\r\n",
      "|                                         |                        |                  N/A |\r\n",
      "+-----------------------------------------+------------------------+----------------------+\r\n",
      "|   1  Tesla T4                       Off |   00000000:00:05.0 Off |                    0 |\r\n",
      "| N/A   37C    P0             25W /   70W |    5719MiB /  15360MiB |      0%      Default |\r\n",
      "|                                         |                        |                  N/A |\r\n",
      "+-----------------------------------------+------------------------+----------------------+\r\n",
      "                                                                                         \r\n",
      "+-----------------------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                              |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\r\n",
      "|        ID   ID                                                               Usage      |\r\n",
      "|=========================================================================================|\r\n",
      "+-----------------------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae578a3",
   "metadata": {
    "papermill": {
     "duration": 0.013069,
     "end_time": "2024-07-29T07:48:27.528128",
     "exception": false,
     "start_time": "2024-07-29T07:48:27.515059",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3519f4a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-29T07:48:27.556597Z",
     "iopub.status.busy": "2024-07-29T07:48:27.555504Z",
     "iopub.status.idle": "2024-07-29T07:48:27.568404Z",
     "shell.execute_reply": "2024-07-29T07:48:27.567506Z"
    },
    "papermill": {
     "duration": 0.029328,
     "end_time": "2024-07-29T07:48:27.570336",
     "exception": false,
     "start_time": "2024-07-29T07:48:27.541008",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "@torch.cuda.amp.autocast()\n",
    "def inference(df, model, device, batch_size=BATCH_SIZE, max_length=MAX_LENGTH):\n",
    "    a_win, b_win, tie = [], [], []\n",
    "\n",
    "    model.eval()\n",
    "    for start_idx in range(0, len(df), batch_size):\n",
    "        end_idx = min(start_idx + batch_size, len(df))\n",
    "        tmp = df.iloc[start_idx:end_idx]\n",
    "        input_ids = tmp[\"input_ids\"].to_list()\n",
    "        attention_mask = tmp[\"attention_mask\"].to_list()\n",
    "        labels = tmp[\"labels\"].to_list()\n",
    "        inputs = pad_without_fast_tokenizer_warning(\n",
    "            tokenizer,\n",
    "            {\"input_ids\": input_ids, \"attention_mask\": attention_mask},\n",
    "            padding=\"longest\",\n",
    "            pad_to_multiple_of=None,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        input_ids = inputs[\"input_ids\"].to(device)\n",
    "        attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "        pad_labels=[]\n",
    "        for label in labels:\n",
    "            label = list(label) + [tokenizer.pad_token_id]*(input_ids[0].shape[0]-label.shape[0])\n",
    "            pad_labels.append(label)\n",
    "        labels = torch.tensor(pad_labels).to(device)\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        proba = torch.softmax(outputs.logits, dim=-1).cpu().numpy()\n",
    "        a_win.extend(proba[:, 0].tolist())\n",
    "        b_win.extend(proba[:, 1].tolist())\n",
    "        tie.extend(proba[:, 2].tolist())\n",
    "    df['winner_model_a'] = a_win\n",
    "    df['winner_model_b'] = b_win\n",
    "    df['winner_tie'] = tie\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8c9b30cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-29T07:48:27.597096Z",
     "iopub.status.busy": "2024-07-29T07:48:27.596830Z",
     "iopub.status.idle": "2024-07-29T07:48:31.259104Z",
     "shell.execute_reply": "2024-07-29T07:48:31.257999Z"
    },
    "papermill": {
     "duration": 3.678074,
     "end_time": "2024-07-29T07:48:31.261275",
     "exception": false,
     "start_time": "2024-07-29T07:48:27.583201",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed time: 3.655832052230835\n"
     ]
    }
   ],
   "source": [
    "st = time.time()\n",
    "\n",
    "data = data.sort_values(\"max_len\", ascending=False)\n",
    "sub_1 = data.iloc[0::2].copy()\n",
    "sub_2 = data.iloc[1::2].copy()\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=2) as executor:\n",
    "    results = executor.map(inference, (sub_1, sub_2), (model_0, model_1), (device0, device1))\n",
    "\n",
    "result_df = pd.concat(list(results), axis=0)\n",
    "proba = result_df[[\"winner_model_a\", \"winner_model_b\", \"winner_tie\"]].values\n",
    "\n",
    "print(f\"elapsed time: {time.time() - st}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "410815f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-29T07:48:31.289119Z",
     "iopub.status.busy": "2024-07-29T07:48:31.288796Z",
     "iopub.status.idle": "2024-07-29T07:48:31.294632Z",
     "shell.execute_reply": "2024-07-29T07:48:31.293619Z"
    },
    "papermill": {
     "duration": 0.021761,
     "end_time": "2024-07-29T07:48:31.296519",
     "exception": false,
     "start_time": "2024-07-29T07:48:31.274758",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working\n"
     ]
    }
   ],
   "source": [
    "cd /kaggle/working/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "047ca423",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-29T07:48:31.323888Z",
     "iopub.status.busy": "2024-07-29T07:48:31.323605Z",
     "iopub.status.idle": "2024-07-29T07:48:31.341269Z",
     "shell.execute_reply": "2024-07-29T07:48:31.340449Z"
    },
    "papermill": {
     "duration": 0.033393,
     "end_time": "2024-07-29T07:48:31.343348",
     "exception": false,
     "start_time": "2024-07-29T07:48:31.309955",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>winner_model_a</th>\n",
       "      <th>winner_model_b</th>\n",
       "      <th>winner_tie</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1233961</td>\n",
       "      <td>0.273839</td>\n",
       "      <td>0.531979</td>\n",
       "      <td>0.194181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>136060</td>\n",
       "      <td>0.124172</td>\n",
       "      <td>0.608815</td>\n",
       "      <td>0.267013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>211333</td>\n",
       "      <td>0.241968</td>\n",
       "      <td>0.558215</td>\n",
       "      <td>0.199817</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  winner_model_a  winner_model_b  winner_tie\n",
       "2  1233961        0.273839        0.531979    0.194181\n",
       "0   136060        0.124172        0.608815    0.267013\n",
       "1   211333        0.241968        0.558215    0.199817"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result_df.loc[:, \"winner_model_a\"] = proba[:, 0]\n",
    "result_df.loc[:, \"winner_model_b\"] = proba[:, 1]\n",
    "result_df.loc[:, \"winner_tie\"] = proba[:, 2]\n",
    "submission_df = result_df[[\"id\", 'winner_model_a', 'winner_model_b', 'winner_tie']]\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "display(submission_df)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 8346466,
     "sourceId": 66631,
     "sourceType": "competition"
    },
    {
     "datasetId": 5460925,
     "sourceId": 9056595,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5461171,
     "sourceId": 9056907,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 148861315,
     "sourceType": "kernelVersion"
    },
    {
     "modelId": 95842,
     "modelInstanceId": 70837,
     "sourceId": 84326,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30699,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 161.362802,
   "end_time": "2024-07-29T07:48:34.379733",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-07-29T07:45:53.016931",
   "version": "2.5.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "01fdf81523fa4c5bb62df78d221ca884": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_7c330cdf0c254aba98235aa57c7ae59f",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_995739acff1948ff9d2f63fa0cf17572",
       "value": " 3/3 [00:00&lt;00:00, 55.57 examples/s]"
      }
     },
     "15280fb8c2ab40ad977aca14d6d43f9e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "469c8c16fc7c4d5a849a53fcc383036c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "7170e7949b7f4dc59b23a09017ba0d64": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "7c330cdf0c254aba98235aa57c7ae59f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9665a67b58e34bb0978678b275e62700": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_15280fb8c2ab40ad977aca14d6d43f9e",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_7170e7949b7f4dc59b23a09017ba0d64",
       "value": "Map: 100%"
      }
     },
     "995739acff1948ff9d2f63fa0cf17572": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "b7ed5a3ce20540cebbe87a8f39e0aa96": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_9665a67b58e34bb0978678b275e62700",
        "IPY_MODEL_cd18920692c7401db2936415429f7ee5",
        "IPY_MODEL_01fdf81523fa4c5bb62df78d221ca884"
       ],
       "layout": "IPY_MODEL_e0e92befac3b4761a2f883835cccd587"
      }
     },
     "cd18920692c7401db2936415429f7ee5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_d8270aeb5e7e459cbb324bb4432e012e",
       "max": 3,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_469c8c16fc7c4d5a849a53fcc383036c",
       "value": 3
      }
     },
     "d8270aeb5e7e459cbb324bb4432e012e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e0e92befac3b4761a2f883835cccd587": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
